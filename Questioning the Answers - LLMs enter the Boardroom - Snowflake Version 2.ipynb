{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "d24wrwrpkn5p7ve6bync",
   "authorId": "4988802824334",
   "authorName": "HENRY.CHIANG@SPGLOBAL.COM",
   "authorEmail": "henry.chiang@spglobal.com",
   "sessionId": "156f3640-9f04-416f-9dbc-f3a6e66436da",
   "lastEditTime": 1739455078781
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "8b3e3fe0-c477-42c5-a3e3-554712bd9647",
   "metadata": {
    "language": "python",
    "name": "py_headline",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "import streamlit as st\nst.markdown(\"\"\"\n<div style=\"background-color: #000020; color: white; text-align: center; padding: 20px\">\n  <h1 style=\"margin: 0; color: white\"><b>Questioning the Answers: LLMs enter the Boardroom</b></h1>\n  <h2 style=\"margin: 0; color: white\"><b></b>Using Gen AI Tools to Harness Alpha from Earnings Calls</h2>\n  <h3 style=\"margin: 0; color: white\"><b>by S&P Global Market Intelligence's Quantitative Research & Solutions (QRS) Group</b></h3>\n</div>\n\"\"\", unsafe_allow_html=True)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "df5cc4ff-46f3-4148-a12f-77f3328b1ad7",
   "metadata": {
    "name": "cell2",
    "collapsed": false
   },
   "source": "Read this QuickStart to setup an environment to run this notebook:"
  },
  {
   "cell_type": "markdown",
   "id": "04b2912a-e563-47a2-8731-7bba38772e76",
   "metadata": {
    "name": "md_1_overview",
    "collapsed": false
   },
   "source": "# 1. Overview\n\nEarnings calls play a pivotal role in shaping investor perceptions. The quality of communication between executives and analysts can significantly influence company performance. On-topic and proactive exeutives, who deliver proactive presentations, anticipate market queries, and provide clear, on-topic answers to analysts’ questions—consistently outperform their peers. Conversely, off-topic and reactive executives, who fail to address analysts’ key inquiries during presentations, and provide off-topic responses—significantly underperform.\n\nExecutives' ability to anticipate investor concerns and maintain a focused dialogue fosters confidence and strategic communication. In contrast, failing to provide clarity when analysts seek additional information can lead to misalignment and breakdowns in transparency. A long (short) portfolio of on-topic and proactive (off-topic and reactive) generates +515bps of annualized alpha.\n\nThis notebook serves as the blueprint for the research detailed in Quantitative Research & Solutions’ recent publication, [\"Questioninig the Answers: LLM's enter the Boardroom.\"](https://www.spglobal.com/market-intelligence/en/news-insights/research/questioning-the-answers-llms-enter-the-boardroom) It analyse executive on-topicness and proactiveness using the analysts questions, executives answers and LLM answers. This research harness alpha using LLM tools, including vector embeddings, vector cosine similarity, and the LLM quesiton answering."
  },
  {
   "cell_type": "markdown",
   "id": "fa040835-3299-4600-b06f-9a47ff3312f7",
   "metadata": {
    "name": "md_3_libraries",
    "collapsed": false
   },
   "source": "# 2. Libraries & User Inputs\nImport libraries required for the workflow\n\n## 2.1 Libraries"
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "py_imports",
    "codeCollapsed": false,
    "collapsed": false
   },
   "source": "import streamlit as st\n\nfrom snowflake.snowpark import functions as snow_funcs\nfrom snowflake.snowpark import Window\nfrom snowflake.snowpark.types import ArrayType, StringType\nfrom snowflake.snowpark.context import get_active_session\n\nsession = get_active_session()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c4187b8f-34a3-402c-8c63-96623f676453",
   "metadata": {
    "name": "md_32_user_inputs",
    "collapsed": false
   },
   "source": "## 2.2 User Inputs\n\nThis research invloves the usage of an embedding model and a completion model, the default models were set to \"snowflake-arctic-embed-m\" for embedding and \"llama3.1-8b\" for completion. This user input section gives you the flexibility to chose your own model for the task."
  },
  {
   "cell_type": "code",
   "id": "fe9edb8d-b06e-4394-acd8-ba517c9b98d6",
   "metadata": {
    "language": "python",
    "name": "py_user_inputs",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Which of the two embedding functions we want to use\nsnf_embed_text_func = \"SNOWFLAKE.CORTEX.EMBED_TEXT_768\" \n\n# Which embedding model we want to use, see https://docs.snowflake.com/en/user-guide/snowflake-cortex/llm-functions#availability \n# for avalible embedding models in Snowflake\nembedding_model = \"snowflake-arctic-embed-m\" \n\n# Which LLM we want to use, see https://docs.snowflake.com/en/user-guide/snowflake-cortex/llm-functions#availability \n# for avalible LLMs in Snowflake\ncompletion_model = \"llama3.1-8b\" \n\n# Name of the databse created in the Setup Snowflake step\nsp_llm_qs_location = \"QRSLLM_POC_DB.QTA_SCHEMA\"\n\n# Name of the shared database created at the \"Request the S&P Global Market Intelligence QuickStart dataset\" step\nsp_qs_share_location = \"SPGLOBALXPRESSCLOUD_SPGMIQRS.XPRESSFEED\"",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f5a4c140-634d-4a3d-9db3-bf2febbbbafc",
   "metadata": {
    "name": "md_2_datasets",
    "collapsed": false
   },
   "source": "# 3. Datasets\n\nThis is a simplified notebook so no data request is needed. The sample data can be found in the Github repository.\n\nHowever, to replicate the full research, you would need the following datasets from Snowflake Marketplace from S&P Global Market Intelligence:\n\n|Name|Description |\n|----|----|\n|[ S&P Capital IQ Financials](https://app.snowflake.com/marketplace/listing/GZT0Z8P3D2N/s-p-global-market-intelligence-s-p-capital-iq-financials)|S&P Capital IQ Financials provides global standardized financial statement data for over 180,000 companies, including over 95,000 active and inactive public companies, and As Reported data for over 150,000 companies. S&P Capital IQ Standardized Financials allows you to extend the scope of your historical analysis and back-testing models with consistent data from all filings of a company's historical financial periods including press releases, original filings, and all restatements.|\n|[Global Events](https://app.snowflake.com/marketplace/listing/GZT0Z8P3D38/s-p-global-market-intelligence-global-events)|The Global Events dataset provides details on upcoming and past corporate events such as earnings calls, shareholder/analyst meetings, expected earnings release dates and more. With deep history back to 2003, clients can leverage this dataset to derive signals and support trading models across asset classes, trading styles and frequencies. This dataset also helps in research & analysis, risk management & compliance, and trade surveillance workflows.|\n|[Machine Readable Transcripts](https://app.snowflake.com/marketplace/listing/GZT0Z8P3D2V/s-p-global-market-intelligence-machine-readable-transcripts)|The Machine Readable Transcripts dataset aggregates data from earnings calls delivered in a machine-readable format for Natural Language Processing (NLP) applications with metadata tagging. Leverage Machine Readable Transcripts to keep track of event information for specific companies including dates, times, dial-in and replay numbers and investor relations contact information. Easily combine data from earnings, M&A, guidance, shareholder, company conference presentations and special calls with traditional datasets to develop proprietary analytics.|"
  },
  {
   "cell_type": "markdown",
   "id": "74eabc40-c3ca-461a-919e-8617ff571f6c",
   "metadata": {
    "name": "cell22",
    "collapsed": false
   },
   "source": "## 3.1 S&P Global Q4 2024 Earnings Call Transcript\nSelect and seperate the prepared remarks, questions and answers sections of the S&P Global Q4 2024 Earnings Call. The prepared remarks and the answers are split into sentences for the RAG workflow."
  },
  {
   "cell_type": "code",
   "id": "75be4dc1-cd84-44ff-91ee-8ccedd7a0227",
   "metadata": {
    "language": "python",
    "name": "cell1",
    "collapsed": false
   },
   "outputs": [],
   "source": "session.sql(f'''SELECT * FROM {sp_llm_qs_location}.SAMPLE_TRANSCRIPT ORDER BY COMPONENTORDER ASC, SENTENCEORDER ASC''')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8846bbd6-0f14-449d-aa4b-452766df70c0",
   "metadata": {
    "name": "cell5",
    "collapsed": false
   },
   "source": "# 4. Working with the Data: Retrival Augmented Generation (RAG)\nRetrieval-Augmented Generation (RAG) is a tool that improves LLM consistency by retrieving relevant information before answering a question.\n\nFor this task the LLM needs to be as consistent as possible in its responses to the analysts’ questions as inconsistency will lead to variations in cosine similarity scores and disrupt feature generation downstream.\n\nTo combat this, we designed a Retrieval-Augmented Generation (RAG) engine that chunks the prepared remarks sentence-by-sentence and retrieves the optimal retrieval percentage of sentences most similar to the question. Inconsistency occurs when the LLM is provided with too little (or too much) context, it becomes uninformed (unspecific). The optimal retrieval percentage for consistency is 60%.\n\nIn the cells below, we vector embed all questions, prepared remark sentences and answer sentences using the snowflake-arctic-m embedding model. Then use the cosine similarity from the (question vs prepared remark sentences) and (question vs answer sentences) to select top 60% most relevant prepared remark and answer sentences to the question from S&P Global Q4 2024 Earnings Call Transcript"
  },
  {
   "cell_type": "code",
   "id": "37260f38-8a8e-4cf3-89db-688de8d7f518",
   "metadata": {
    "language": "python",
    "name": "cell3",
    "collapsed": false
   },
   "outputs": [],
   "source": "all_component_df = session.table(f\"{sp_llm_qs_location}.SAMPLE_TRANSCRIPT\")\n\npppSentences = all_component_df.filter(snow_funcs.col('transcriptComponentTypeId') == 2) \nquestions = all_component_df.filter(snow_funcs.col('transcriptComponentTypeId') == 3)\nanswerSentences = all_component_df.filter(snow_funcs.col('transcriptComponentTypeId') == 4) \n\nst.dataframe(pppSentences.limit(5))\nst.dataframe(questions.limit(5))\nst.dataframe(answerSentences.limit(5))",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c677af3f-f8a9-49f0-84b7-908ddf55dd90",
   "metadata": {
    "name": "cell6",
    "collapsed": false
   },
   "source": "## 4.2 Vector Embedding\nTransform the questions, prepared remark sentences and answer sentences into numerical\nrepresentations using the snowflake-arctic-m embedding model.\n\n### 4.2.2 Apply Embedding to Transcript Components"
  },
  {
   "cell_type": "code",
   "id": "125b2a68-744f-43ad-bb55-12c61dbcd2db",
   "metadata": {
    "language": "python",
    "name": "cell7",
    "collapsed": false
   },
   "outputs": [],
   "source": "snf_embed_text = snow_funcs.function(snf_embed_text_func)\n\npppSentences_with_embeddings = pppSentences.withColumn(\"sentenceVec\", snf_embed_text(snow_funcs.lit(embedding_model)\n                                                                                     ,pppSentences[\"PROCESSEDTEXT\"]))\nquestions_with_embeddings = questions.withColumn(\"questionVec\", snf_embed_text(snow_funcs.lit(embedding_model),\n                                                                                    questions[\"PROCESSEDTEXT\"]))\nanswerSentences_with_embeddings = answerSentences.withColumn(\"sentenceVec\",  snf_embed_text(snow_funcs.lit(embedding_model)\n                                                                                            ,answerSentences[\"PROCESSEDTEXT\"]))\n\npppSentences_with_embeddings.write.save_as_table(f\"{sp_llm_qs_location}.pppSentencesVec\", mode=\"overwrite\",table_type='transient')\nquestions_with_embeddings.write.save_as_table(f\"{sp_llm_qs_location}.questionsVec\", mode=\"overwrite\", table_type='transient')\nanswerSentences_with_embeddings.write.save_as_table(f\"{sp_llm_qs_location}.answerSentencesVec\", mode=\"overwrite\", table_type='transient')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1ed3cf27-1191-4415-86a5-e739c5b7bac3",
   "metadata": {
    "name": "cell8",
    "collapsed": false
   },
   "source": "# 4.3 Cosine Similarity\nTo determine semantic closeness, we vector-embed the question-and-answer texts and calculate a cosine similarity score between the two vectors.\nFor example, A and B each represent a vector such that:\n\n\n$$\tA = [a_1,a_2,… a_n] $$\n$$\tB = [b_1,b_2,… b_n] $$\n\n\nThe cosine similarity formula between vectors A and B is:\n\n$$\\text{Cosine Similarity} = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\cdot \\|\\mathbf{B}\\|}$$\n\n, where A⋅B is the dot product of the vectors, and |A|⋅|B| is the product of each vector's magnitude.\nThe result ranges from -1 to 1, where:\n\n\t-1: Vectors are opposite.\n\t0: Vectors are unrelated.\n\t1: Vectors are identical.\n\n### 4.3.2 Apply Cosine Similarity to Question & Answer Vector Embeddings\nThe cell below creates question and answer pair by collecting all the answer sentences whose componentOrder is between the currentQuestionComponentOrder and the nextQuestionComponentOrder. Then the the consine similarity was applied to the question and answer sentence vector embeddings."
  },
  {
   "cell_type": "code",
   "id": "b139c451-e0ba-4646-906d-0cfe65e55ecc",
   "metadata": {
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": "df = session.sql(f\"\"\"\n\nwith q as(\n    select \n        callDate, enteredDate, fiscalYearQuarter, calendarYearQuarter,\n        tradingItemId, companyId, companyName, headline, transcriptId,\n        speakerTypeName as questionSpeakerTypeName, transcriptPersonName as questionTranscriptPersonName,\n        transcriptPersonId as questionTranscriptPersonId, proId as questionProId, transcriptComponentTypeId as questionTranscriptComponentTypeId,\n        transcriptComponentId as questionTranscriptComponentId, componentOrder as currentQuestionOrder\n        , lead(componentOrder) over (partition by tradingItemId, transcriptId order by componentOrder) as nextQuestionOrder\n        , processedText as question, questionVec\n    from {sp_llm_qs_location}.questionsVec\n    where 1=1\n        and transcriptComponentTypeId = 3\n)\nselect \n    q.callDate, q.enteredDate, q.fiscalYearQuarter, q.calendarYearQuarter,\n    q.tradingItemId, q.companyId, q.companyName, q.headline, q.transcriptId,\n    q.questionSpeakerTypeName, q.questionTranscriptPersonName, q.questionTranscriptPersonId, q.questionProId, \n    a.speakerTypeName as answerSpeakerTypeName, a.transcriptPersonName as answerTranscriptPersonName, a.transcriptPersonId as answerTranscriptPersonId, a.proId as answerProId, \n    q.questionTranscriptComponentTypeId, a.transcriptComponentTypeId as answerTranscriptComponentTypeId,\n    q.questionTranscriptComponentId, a.transcriptComponentId as answerTranscriptComponentId,\n    q.currentQuestionOrder, q.nextQuestionOrder, a.componentOrder as answerOrder, a.sentenceOrder as answerSentenceOrder,\n    q.question, a.componentText as answer, a.processedText as answerSentence, q.questionVec, a.sentenceVec as answerSentenceVec\n    , VECTOR_COSINE_SIMILARITY(questionVec, sentenceVec) AS cosine_similarity\nfrom q\n    join {sp_llm_qs_location}.answerSentencesVec a on q.tradingItemId = a.tradingItemId and q.transcriptId = a.transcriptId and a.componentOrder between q.currentQuestionOrder and ifnull(q.nextQuestionOrder, 10000)\norder by q.nextQuestionOrder asc, a.componentOrder asc, a.sentenceOrder asc\n\n\"\"\")\n\ndf.write.save_as_table(f\"{sp_llm_qs_location}.TranscriptComponents_qaPairCos\", mode=\"overwrite\", table_type='transient')\ndf.limit(5)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "851dc868-4761-499a-aa4d-a9ed379968e4",
   "metadata": {
    "name": "cell10",
    "collapsed": false
   },
   "source": "### 4.3.3 Apply Cosine Similarity to Question & Prepared Remarks Vector Embeddings\nThe cell below pairs all prepared remarks sentences to questions. Then the the consine similarity was applied to the question and prepared remarks sentence vector embeddings."
  },
  {
   "cell_type": "code",
   "id": "8bb58b83-2d2e-456f-b776-55f6aa4a8021",
   "metadata": {
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": "df = session.sql(f\"\"\"\n\nwith q as(\n    select \n        callDate, enteredDate, fiscalYearQuarter, calendarYearQuarter,\n        tradingItemId, companyId, companyName, headline, transcriptId,\n        speakerTypeName as questionSpeakerTypeName, transcriptPersonName as questionTranscriptPersonName,\n        transcriptPersonId as questionTranscriptPersonId, proId as questionProId, transcriptComponentTypeId as questionTranscriptComponentTypeId,\n        transcriptComponentId as questionTranscriptComponentId, componentOrder as currentQuestionOrder, lead(componentOrder) over (partition by tradingItemId, transcriptId order by componentOrder) as nextQuestionOrder, \n        processedText as question, questionVec\n    from {sp_llm_qs_location}.questionsVec\n    where 1=1\n        and transcriptComponentTypeId = 3\n)\nselect \n    q.*, \n    p.componentOrder as executiveRemarkComponentOrder, p.sentenceOrder as executiveRemarkSentenceOrder,\n    p.componentText as executiveRemark, p.processedText as executiveSentence, p.sentenceVec as executiveVec,\n    VECTOR_COSINE_SIMILARITY(questionVec, p.sentenceVec) as similarity\nfrom q\n    join {sp_llm_qs_location}.pppSentencesVec p on q.tradingItemId = p.tradingItemId and q.transcriptId = p.transcriptId\n\norder by q.nextQuestionOrder asc, p.componentOrder asc, p.sentenceOrder asc\n\n\"\"\")\n\ndf.write.save_as_table(f\"{sp_llm_qs_location}.TranscriptComponents_pppQPairCos\", mode=\"overwrite\", table_type='transient')\ndf.limit(5)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "329279ff-260d-4d0e-bebe-e426f0b68612",
   "metadata": {
    "name": "cell12",
    "collapsed": false
   },
   "source": "## 4.4 Top 60% Sentences\nUtilizing the top 60% of prepared remarks identified as generating the most consistent LLM output. For further details on the experiment, please refer to the 'LLM Robustness Check' section in the whitepaper.\n\n### 4.4.2 Concat Top 60% Answer Sentences\nAfter selecting the top 60% most similar answer sentences, we concat the answer sentences on the question level."
  },
  {
   "cell_type": "code",
   "id": "a2bd1ccb-788b-4321-a470-2e7f8223e970",
   "metadata": {
    "language": "python",
    "name": "cell13",
    "collapsed": false
   },
   "outputs": [],
   "source": "\ndf = session.sql(f'''\n          \n    with qaSimilarityRank as(\n    select \n        *, \n        ROW_NUMBER() OVER (PARTITION BY tradingItemId, transcriptId, currentQuestionOrder, currentQuestionOrder ORDER BY cosine_similarity desc) AS similarityRank,\n        COUNT(*) OVER (PARTITION BY tradingItemId, transcriptId, currentQuestionOrder, currentQuestionOrder) AS answerSentencesCount \n    from {sp_llm_qs_location}.TranscriptComponents_qaPairCos \n    ), \n    questionWithSixtyPercentAnswers as(\n\n    select \n        *, \n        case \n            when answerSentencesCount = 1 then 1\n            when similarityRank <= answerSentencesCount * 0.67 then 1\n            else 0\n        end as toKeepFlag\n    from qaSimilarityRank\n\n    ),\n    qaPair60 as(\n    select\n        callDate, enteredDate, fiscalYearQuarter, calendarYearQuarter, tradingItemId, companyId, companyName\n        , headline, transcriptId, questionSpeakerTypeName, questionTranscriptPersonName, questionTranscriptPersonId\n        , questionProId, answerSpeakerTypeName, answerTranscriptPersonName, answerTranscriptPersonId, answerProId\n        , questionTranscriptComponentTypeId, answerTranscriptComponentTypeId, questionTranscriptComponentId\n        , answerTranscriptComponentId, currentQuestionOrder, nextQuestionOrder, answerOrder, question, answer, \n        ARRAY_TO_STRING(ARRAY_AGG(answerSentence), ' ') AS sixtyPercentAnswer\n    from (select * from questionWithSixtyPercentAnswers where toKeepFlag = 1 order by currentQuestionOrder asc, answerOrder asc, answerSentenceOrder asc)\n    group by callDate, enteredDate, fiscalYearQuarter, calendarYearQuarter, tradingItemId, companyId, companyName, headline, transcriptId, questionSpeakerTypeName, questionTranscriptPersonName, questionTranscriptPersonId, questionProId, answerSpeakerTypeName, answerTranscriptPersonName, answerTranscriptPersonId, answerProId, questionTranscriptComponentTypeId, answerTranscriptComponentTypeId, questionTranscriptComponentId, answerTranscriptComponentId, currentQuestionOrder, nextQuestionOrder, answerOrder, question, answer)\n    select \n        callDate, enteredDate, fiscalYearQuarter, calendarYearQuarter, tradingItemId, companyId, companyName, headline, transcriptId, questionSpeakerTypeName, questionTranscriptPersonName, questionTranscriptPersonId, questionProId, answerSpeakerTypeName, answerTranscriptPersonName, answerTranscriptPersonId, answerProId, questionTranscriptComponentTypeId, answerTranscriptComponentTypeId, questionTranscriptComponentId, answerTranscriptComponentId, currentQuestionOrder, nextQuestionOrder, answerOrder, \n        question, answer\n        , SNOWFLAKE.CORTEX.COUNT_TOKENS('{completion_model}',answer) as answerTokenCount\n        , sixtyPercentAnswer\n        , SNOWFLAKE.CORTEX.COUNT_TOKENS('{completion_model}',sixtyPercentAnswer) as sixtyPercentAnswerTokenCount\n    from qaPair60\n    order by currentQuestionOrder asc, answerOrder asc\n              \n''')\n\ndf.write.save_as_table(f\"{sp_llm_qs_location}.TranscriptComponents_qaPairTop60\", mode=\"overwrite\", table_type='transient')\ndf.limit(5)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5e3d00a6-e39b-46a4-abf3-76b8e86bdcef",
   "metadata": {
    "name": "cell11",
    "collapsed": false
   },
   "source": "### 4.4.3 Concat Top 60% Prepared Remarks Sentences\nSimilarly, after selecting the top 60% most similar prepared remarks sentences, we concat the prepared remarks sentences on the question level."
  },
  {
   "cell_type": "code",
   "id": "6c64961c-20ce-4fb6-ad4a-528a5e8fff56",
   "metadata": {
    "language": "python",
    "name": "cell15",
    "collapsed": false
   },
   "outputs": [],
   "source": "\ndf = session.sql(f'''\n\nwith qaSimilarityRank as(\nselect \n    *, \n    ROW_NUMBER() OVER (PARTITION BY tradingItemId, transcriptId, currentQuestionOrder ORDER BY similarity desc) AS similarityRank,\n    COUNT(*) OVER (PARTITION BY tradingItemId, transcriptId, currentQuestionOrder) AS executiveRemarksSentencesCount\nfrom {sp_llm_qs_location}.TranscriptComponents_pppQPairCos\n),\n\npppQPairTop60 as(\nselect \n    *, \n    case \n        when executiveRemarksSentencesCount = 1 then 1\n        when similarityRank <= executiveRemarksSentencesCount * 0.67 then 1\n        else 0\n    end as toKeepFlag\nfrom qaSimilarityRank),\n\npresenterLevelSixtyPercentPPP as(\nselect\n    callDate, enteredDate, fiscalYearQuarter, calendarYearQuarter, tradingItemId, companyId, companyName, headline, transcriptId, questionSpeakerTypeName, questionTranscriptPersonName, questionTranscriptPersonId, questionProId, questionTranscriptComponentTypeId, questionTranscriptComponentId, currentQuestionOrder, nextQuestionOrder, executiveRemarkComponentOrder, question, executiveRemark, \n    ARRAY_TO_STRING(ARRAY_AGG(executiveSentence), ' ') AS sixtyPercentExecutiveRemark\nfrom (select * from pppQPairTop60 where toKeepFlag = 1 order by currentQuestionOrder asc, executiveRemarkComponentOrder asc, executiveRemarkSentenceOrder asc)\ngroup by callDate, enteredDate, fiscalYearQuarter, calendarYearQuarter, tradingItemId, companyId, companyName, headline, transcriptId, questionSpeakerTypeName, questionTranscriptPersonName, questionTranscriptPersonId, questionProId, questionTranscriptComponentTypeId, questionTranscriptComponentId, currentQuestionOrder, nextQuestionOrder, executiveRemarkComponentOrder, question, executiveRemark\norder by currentQuestionOrder asc,  executiveRemarkComponentOrder asc),\n\npppQPairTop60Concat as(\nselect \n\n    callDate, enteredDate, fiscalYearQuarter, calendarYearQuarter, tradingItemId, companyId, companyName, headline, transcriptId, questionSpeakerTypeName, questionTranscriptPersonName, questionTranscriptPersonId, questionProId, questionTranscriptComponentTypeId, questionTranscriptComponentId, currentQuestionOrder, nextQuestionOrder, question,\n    ARRAY_TO_STRING(ARRAY_AGG(executiveRemark), '\\n\\n') AS questionLevelConcatenatedRawExecutiveRemark,\n    ARRAY_TO_STRING(ARRAY_AGG(sixtyPercentExecutiveRemark), '\\n\\n') AS questionLevelConcatenatedSixtyPercentExecutiveRemark\nfrom (select * from presenterLevelSixtyPercentPPP order by currentQuestionOrder asc,  executiveRemarkComponentOrder asc)\ngroup by callDate, enteredDate, fiscalYearQuarter, calendarYearQuarter, tradingItemId, companyId, companyName, headline, transcriptId, questionSpeakerTypeName, questionTranscriptPersonName, questionTranscriptPersonId, questionProId, questionTranscriptComponentTypeId, questionTranscriptComponentId, currentQuestionOrder, nextQuestionOrder, question\n)\n\nselect \n    callDate, enteredDate, fiscalYearQuarter, calendarYearQuarter, tradingItemId, companyId, companyName, headline, transcriptId, questionSpeakerTypeName, questionTranscriptPersonName, questionTranscriptPersonId, questionProId, questionTranscriptComponentTypeId, questionTranscriptComponentId, currentQuestionOrder, nextQuestionOrder, question,\n    questionLevelConcatenatedRawExecutiveRemark\n    , SNOWFLAKE.CORTEX.COUNT_TOKENS('{completion_model}',questionLevelConcatenatedRawExecutiveRemark) as questionLevelConcatenatedRawExecutiveRemarkTokenCount, questionLevelConcatenatedSixtyPercentExecutiveRemark\n    , SNOWFLAKE.CORTEX.COUNT_TOKENS('{completion_model}',questionLevelConcatenatedSixtyPercentExecutiveRemark) as questionLevelConcatenatedSixtyPercentExecutiveRemarkTokenCount\nfrom pppQPairTop60Concat\norder by currentQuestionOrder asc\n\n''')\n\ndf.write.save_as_table(f\"{sp_llm_qs_location}.TranscriptComponents_pppQPairTop60\", mode=\"overwrite\", table_type='transient')\ndf.limit(5)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c007d216-3383-4871-add9-dee702a423d1",
   "metadata": {
    "name": "cell16",
    "collapsed": false
   },
   "source": "# 5. Working with the Data: LLM Ready Data\n\nUsing a LLM to answer analysts questions based only on the prepared remarks and the previous questions and answers will give an indication if executives are proactive.\n\n## 5.1 Using Snowflake Cortex AI\nWhen calling the Snowflake Cortex COMPLETE function, messages are organized into distinct roles—system, user, and assistant—to structure and guide interactions. Each role serves a specific purpose:\n\nSystem: Provides instructions that define the context or behavior of the model. It's like setting the rules or tone for the conversation. Example: \"You are a helpful assistant that answers questions about technology in a concise manner.\"\n\nUser: Represents the input or queries made by the person interacting with the model. These are the prompts or requests that the model responds to. Example: \"What is the purpose of the OpenAI API?\"\n\nAssistant: Reflects the model's response to the user's query, shaped by the system's instructions and the user's input. Example: \"The OpenAI API is designed to enable developers to integrate language models into their applications for tasks like answering questions, generating content, and more.\"\n\nIn our research, executive prepared remarks are labelled as assistant messages, analyst's questions as User messages and executive answers as Assistant messages\n\n### 5.1.1 Construct COMPLETE Assistant Message with QA Pair Snippet"
  },
  {
   "cell_type": "code",
   "id": "0e0352cd-0393-4121-8c66-fc09abef24a3",
   "metadata": {
    "language": "python",
    "name": "cell17",
    "collapsed": false
   },
   "outputs": [],
   "source": "df = session.sql(f'''\n\nwith questionWithSixtyPercentAnswersPromptSnippet as (\n\nselect\n    *, \n    OBJECT_CONSTRUCT('role', 'user', 'content', REPLACE(question, '\\r', '')) as questionPromptSnippet,\n    OBJECT_CONSTRUCT('role', 'assistant', 'content', REPLACE(sixtyPercentAnswer, '\\r', '')) as answerPromptSnippet\n    \nfrom {sp_llm_qs_location}.TranscriptComponents_qaPairTop60)\nselect \n    *, ARRAY_CONSTRUCT(questionPromptSnippet, answerPromptSnippet) as questionAnswerPairPromptSnippet \nfrom questionWithSixtyPercentAnswersPromptSnippet\norder by currentQuestionOrder asc, answerOrder asc\n\n''')\n\n\ndf.write.save_as_table(f\"{sp_llm_qs_location}.TranscriptComponents_pppQPairTop60AnswerConcat\", mode=\"overwrite\", table_type='transient')\ndf.limit(5)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d6177f2a-94ac-47be-bff5-8f8489e60aaa",
   "metadata": {
    "name": "cell18",
    "collapsed": false
   },
   "source": "### 5.1.2 Construct Snowflake Cortex COMPLETE Assistant Message with Prepared Remarks Snippets"
  },
  {
   "cell_type": "code",
   "id": "958604cb-79fb-4e37-8f7e-cac7c46540c3",
   "metadata": {
    "language": "python",
    "name": "cell19",
    "collapsed": false
   },
   "outputs": [],
   "source": "df = session.sql(f'''\n\nselect\n    *,\n    OBJECT_CONSTRUCT('role', 'user', 'content', REPLACE(question, '\\r', '')) as questionPromptSnippet,\n    OBJECT_CONSTRUCT('role', 'assistant', 'content', REPLACE(questionLevelConcatenatedSixtyPercentExecutiveRemark, '\\r', '')) as pppPromptSnippet\nfrom {sp_llm_qs_location}.TranscriptComponents_pppQPairTop60\n\n''')\n\ndf.write.save_as_table(f\"{sp_llm_qs_location}.TranscriptComponents_pppQPairTop60PPPConcat\", mode=\"overwrite\", table_type='transient')\ndf.limit(5)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3e81878a-bca1-49bc-a31b-db80d2c5ceb4",
   "metadata": {
    "name": "cell20",
    "collapsed": false
   },
   "source": "## 5.2 Collect All Messages for Cortex COMPLETE\nAll question pairs with prepare remarks and answers come together to form an LLM prompt following the iterative process such that:\n\n1. 'user': 'From the perspective of a top executive, please answer the following question raised by a financial analyst during an earnings conference call. Knowledge cutoff date: '  \n2. 'assistant': 60% prepared remarks  \n3. 'user': question 1  \n4. 'assistant': 60% answer 1  \n5. ...  \n6. ...  \n7. 'user': question n  \n\n### 5.2.1 LLM Ready Prompt Messages\nIn the dataframe below, the prompt column has all the messages in 1 list. This is the prompt for the LLM."
  },
  {
   "cell_type": "code",
   "id": "747e8ae3-1b7c-4bd6-8344-4fc90e778ec0",
   "metadata": {
    "language": "python",
    "name": "cell21",
    "collapsed": false
   },
   "outputs": [],
   "source": "df = session.sql(f'''\n    with pppQPairTop60PPPConcat\n    as\n    (\n    select \n         a.callDate, a.tradingItemId, a.transcriptId, a.headline, \n        a.questionTranscriptPersonName, a.questionTranscriptPersonId, a.questionProId,\n        a.answerTranscriptPersonName, a.answerTranscriptPersonId, a.answerProId,\n        a.questionTranscriptComponentId, a.answerTranscriptComponentId,\n        a.question, a.answer,\n        ARRAY_CONSTRUCT(object_construct('role', 'user', 'content', concat('From the perspective of a top executive, please answer the following question raised by a financial analyst during an earnings conference call. Knowledge cutoff date: ', cast(a.callDate as string)))) as initPrompt,\n       a.questionPromptSnippet,\n       ARRAY_FLATTEN(ARRAY_AGG(a.questionAnswerPairPromptSnippet) OVER (PARTITION BY a.TRANSCRIPTID ORDER BY a.CURRENTQUESTIONORDER \n                                                        ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING)) as concatenatedPredecessors\n     from {sp_llm_qs_location}.TranscriptComponents_pppQPairTop60AnswerConcat a\n    )\n    select a.* \n        , ARRAY_CAT(a.initPrompt, ARRAY_CAT([ppp.pppPromptSnippet], ARRAY_CAT(a.concatenatedPredecessors, [a.questionPromptSnippet]))) as prompt\n    from \n    pppQPairTop60PPPConcat a\n    join {sp_llm_qs_location}.TranscriptComponents_pppQPairTop60PPPConcat as ppp on a.tradingItemId = ppp.tradingItemId and a.transcriptId = ppp.transcriptId and a.questionTranscriptComponentId = ppp.questionTranscriptComponentId\n    order by a.questionTranscriptComponentId asc, a.answerTranscriptComponentId asc\n''')\n\ndf.write.save_as_table(f\"{sp_llm_qs_location}.TranscriptComponents_targetLLMReadyPrompt\", mode=\"overwrite\", table_type='transient')\ndf.limit(5)\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2ddb0cf0-76c9-49f2-bab2-41de606b73ce",
   "metadata": {
    "name": "md_7_llm_ready_data",
    "collapsed": false
   },
   "source": "# 6. Working with the Data: LLM Ready Data\n\nUsing a LLM to answer analysts questions based only on the prepared remarks and the previous questions and answers will give an indication if executives are proactive.\n\n## 6.1 Using Snowflake Cortex AI\nWhen calling the Snowflake Cortex COMPLETE function, messages are organized into distinct roles—system, user, and assistant—to structure and guide interactions. Each role serves a specific purpose:\n\nSystem: Provides instructions that define the context or behavior of the model. It's like setting the rules or tone for the conversation. Example: \"You are a helpful assistant that answers questions about technology in a concise manner.\"\n\nUser: Represents the input or queries made by the person interacting with the model. These are the prompts or requests that the model responds to. Example: \"What is the purpose of the OpenAI API?\"\n\nAssistant: Reflects the model's response to the user's query, shaped by the system's instructions and the user's input. Example: \"The OpenAI API is designed to enable developers to integrate language models into their applications for tasks like answering questions, generating content, and more.\"\n\nIn our research, executive prepared remarks are labelled as assistant messages, analyst's questions as User messages and executive answers as Assistant messages"
  },
  {
   "cell_type": "markdown",
   "id": "c509c9a6-2350-4293-9acc-1fb5c540830e",
   "metadata": {
    "name": "md_73_collect_COMPLETE_responses",
    "collapsed": false
   },
   "source": "### 6.1.1 Apply LLM Completion\nWe apply the SNOWFLAKE.CORTEX.COMPLETE function on the prompt column using the model defined by `completion_model`and collect the LLM response."
  },
  {
   "cell_type": "code",
   "id": "30081e59-9940-4304-8ed3-62f2b8374696",
   "metadata": {
    "language": "python",
    "name": "cell4",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "df = session.sql(f\"\"\"\n\n    select *,  SNOWFLAKE.CORTEX.COMPLETE('{completion_model}', prompt, {{'temperature': 0}}) as LLMAnswer \n    from {sp_llm_qs_location}.TranscriptComponents_targetLLMReadyPrompt\n          \n\"\"\")\n\ndf.write.save_as_table(f\"{sp_llm_qs_location}.TranscriptComponents_targetLLMAnswer\", mode=\"overwrite\", table_type='transient')\ndf.limit(5)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0035b140-240c-45bd-8d0a-e51764d0a3eb",
   "metadata": {
    "name": "md_732_clean_up_LLM_responses",
    "collapsed": false
   },
   "source": "### 6.1.2 Clean Up LLM Response\n\nExtract only the actual message from the LLM from the responses"
  },
  {
   "cell_type": "code",
   "id": "62333d10-d5e7-48fa-8e0d-0aa93fb66e10",
   "metadata": {
    "language": "python",
    "name": "py_extract_message"
   },
   "outputs": [],
   "source": "df = session.sql(f\"\"\"\n\n    select \n        callDate, tradingItemId, transcriptId, headline, \n        questionTranscriptPersonName, questionTranscriptPersonId, questionProId, answerTranscriptPersonName, answerTranscriptPersonId, answerProId,\n        questionTranscriptComponentId, answerTranscriptComponentId, question, answer,\n        REPLACE(LLMANSWER: \"choices\"[0]: \"messages\", '\"', '') as cleanLLMAnswer \n    from {sp_llm_qs_location}.TranscriptComponents_targetLLMAnswer\n          \n\"\"\")\n\ndf.write.save_as_table(f\"{sp_llm_qs_location}.TranscriptComponents_targetLLMAnswer_Clean\", mode=\"overwrite\", table_type='transient')\ndf.limit(5)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fb29bb53-5a68-4d00-978a-6302b1eb16a8",
   "metadata": {
    "name": "md_74_summarize_text",
    "collapsed": false
   },
   "source": "## 6.2 Summarize Text\n\nUse the Snowflake Cortex Summarize function to summarize the question, answer and LLM answer"
  },
  {
   "cell_type": "code",
   "id": "6d5c15c2-52f1-4dc0-9a7d-2ea683ec2391",
   "metadata": {
    "language": "python",
    "name": "py_summarize",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "df = session.sql(f\"\"\"\n\n    select \n        *, \n        SNOWFLAKE.CORTEX.SUMMARIZE(question) as summarizeQuestion,\n        SNOWFLAKE.CORTEX.SUMMARIZE(answer) as summarizeAnswer,\n        SNOWFLAKE.CORTEX.SUMMARIZE(cleanLLMAnswer) as summarizeCleanLLMAnswer,\n    from {sp_llm_qs_location}.TranscriptComponents_targetLLMAnswer_Clean\n          \n\"\"\")\n\ndf.write.save_as_table(f\"{sp_llm_qs_location}.TranscriptComponents_targetLLMAnswer_Summarize\", mode=\"overwrite\", table_type='transient')\ndf.limit(5)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cde8c8fb-317d-40ec-b17b-a7efc118fc78",
   "metadata": {
    "name": "md_8_factor_construction",
    "collapsed": false
   },
   "source": "# 7. Working with the Data: Factor Construction\n## 7.1 Executive On/Off Topic Factor\nWhen an executive answer is semantically similar (dissimilar) to the analyst’s question, it suggests that the answer uses language and concepts similar to (different from) the analyst question, indicating it is on-topic (off-topic). To determine semantic closeness, we vector-embed the question-and-answer texts and calculate a cosine similarity score between the two vectors.\n\n### 7.1.1 Question vs Executive Answer Cosine Similarity"
  },
  {
   "cell_type": "code",
   "id": "ffc83173-e945-42e1-a8ce-3fe2785bd834",
   "metadata": {
    "language": "python",
    "name": "py_q_v_a_cosine_sim",
    "collapsed": false
   },
   "outputs": [],
   "source": "df = session.sql(f'''\n\n    with vec as(\n    select *\n        , {snf_embed_text_func}('{embedding_model}', summarizeQuestion) as questionVec\n        , {snf_embed_text_func}('{embedding_model}', summarizeAnswer) as answerVec \n    from {sp_llm_qs_location}.TranscriptComponents_targetLLMAnswer_Summarize\n    )\n    select *, VECTOR_COSINE_SIMILARITY(questionVec, answerVec) as execOnOffTopicFactor from vec \n               \n''')\n\ndf.write.save_as_table(f\"{sp_llm_qs_location}.TranscriptComponents_execOnOffTopicFactor\", mode=\"overwrite\", table_type='transient')\ndf.limit(5)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "38be14f8-7a72-4aab-984c-43aa3e76ff97",
   "metadata": {
    "name": "md_812_mean_on_off_topic",
    "collapsed": false
   },
   "source": "### 7.1.2 Transcript Mean Executive On/Off Topic Factor\nCosine similarity scores are averaged at the transcript level. A high (low) Cosine Similarity Score indicates an On (Off) Topic Executive.\n\n"
  },
  {
   "cell_type": "code",
   "id": "9c2f2ed4-20eb-4ff6-a527-05c9362b066f",
   "metadata": {
    "language": "sql",
    "name": "sql_mean_on_off_topic"
   },
   "outputs": [],
   "source": "select avg(execOnOffTopicFactor) as transcriptLevelExecOnOffTopicFactor \nfrom {{sp_llm_qs_location}}.TranscriptComponents_execOnOffTopicFactor",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "916210b3-8d62-4bac-9a1a-13fbc0d08ffc",
   "metadata": {
    "name": "md_82_executive_pro_re_factor",
    "collapsed": false
   },
   "source": "## 7.2 Executive Proactive/Reactive Factor\n### 7.2.1 Question vs LLM Answer Cosine Similarity\nSince the LLM answers only within the context of information provided in the prepared remarks, a high (low) cosine similarity score indicates that the LLM answers are semantically similar (dissimilar) the questions, reflecting the executives are proactive (reactive)."
  },
  {
   "cell_type": "code",
   "id": "06d679e9-5849-44cd-b73a-595e4a9df160",
   "metadata": {
    "language": "python",
    "name": "py_q_llmA_cosine_sim"
   },
   "outputs": [],
   "source": "df = session.sql(f'''\n\n    with vec as(\n    select \n        *, \n        {snf_embed_text_func}('{embedding_model}', summarizeQuestion) as questionVec, \n        {snf_embed_text_func}('{embedding_model}', summarizeCleanLLMAnswer) as LLMAnswerVec \n    from {sp_llm_qs_location}.TranscriptComponents_targetLLMAnswer_Summarize\n    )\n    select *, VECTOR_COSINE_SIMILARITY(questionVec, LLMAnswerVec) as execProactiveReactiveFactor from vec \n               \n''')\n\ndf.write.save_as_table(f\"{sp_llm_qs_location}.TranscriptComponents_execProactiveReactiveFactor\", mode=\"overwrite\", table_type='transient')\ndf.limit(5)\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5999994a-81e2-428f-ab24-4a5eeb72e150",
   "metadata": {
    "name": "md_822_mean_pro_re_factor",
    "collapsed": false
   },
   "source": "### 7.2.2 Transcript Mean Executive Proactive/Reactive Factor\nSimilar to the construction of the Executive On/Off Topic factor, both the LLM answers and questions are summarized, vector-embedded and cosine similarity scores are averaged at the transcript level."
  },
  {
   "cell_type": "code",
   "id": "79c48dfb-ca72-41ba-af86-d0f5975d6c48",
   "metadata": {
    "language": "sql",
    "name": "sql_mean_pro_re_factor"
   },
   "outputs": [],
   "source": "select avg(execProactiveReactiveFactor) as transcriptLevelexecProactiveReactiveFactor \nfrom {{sp_llm_qs_location}}.TranscriptComponents_execProactiveReactiveFactor",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9e4c3c21-f09e-4994-aa9a-1db01028f0d3",
   "metadata": {
    "name": "md_9_result_summary",
    "collapsed": false
   },
   "source": "# 8. Results & Summary\nThis research underscores the significant impact of executive communication styles during earnings calls on firm performance. Proactive executives who anticipate market concerns and provide concise, on-topic responses foster transparency, aligning with investor expectations and driving superior returns. The findings demonstrate that firms with Efficient Communicators achieve statistically significant outperformance, while Total Redirectors suffer from diminished confidence and underperformance. These insights validate the critical role of strategic communication in shaping investor perceptions and influencing market outcomes.\n\nAdvanced analytical tools, such as vector embeddings and cosine similarity metrics, enable nuanced evaluations of executive-analyst interactions, revealing measurable performance effects across different communication styles. While large language models (LLMs) enhance feature extraction, challenges like forward-looking bias and inconsistency highlight the need for caution in time-sensitive tasks. Overall, the integration of proactive, clear, and relevant communication strategies remains paramount in fostering investor trust and maximizing financial success in a competitive marketplace."
  }
 ]
}